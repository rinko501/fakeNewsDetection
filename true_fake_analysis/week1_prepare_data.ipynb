{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):  # remove puctuation\n",
    "    s = re.sub('[^0-9\\w\\d\\s]+', '', s).replace('\\r',\" \").replace('\\n',\" \").replace('\\xa0',' ')\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './../dataset_a/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the positive and negative samples\n",
    "All_Fake_Data = []\n",
    "All_True_Data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fake news length: 490\n",
      "all true news length: 490\n",
      "OK_1\n"
     ]
    }
   ],
   "source": [
    "# 1. get fakeNewsDatasets(celebrity)\n",
    "for data_set in ['celebrityDataset/', 'fakeNewsDataset/']:\n",
    "    data_path_1 = data_file + 'fakeNewsDatasets(celebrity)/' + data_set\n",
    "    data_path_1_fake = data_path_1 + 'fake/'\n",
    "    data_path_1_true = data_path_1 + 'legit/'\n",
    "    # get fake data\n",
    "    for file in os.listdir(data_path_1_fake):  \n",
    "        file_path = data_path_1_fake + file\n",
    "        data = open(file_path, encoding='utf-8', mode='r') \n",
    "        try:\n",
    "            data = data.readlines()\n",
    "        except:\n",
    "            continue\n",
    "        one_news = []\n",
    "        for line in data:\n",
    "            line = clean_str(line.strip('\\n\\r')).split(' ')\n",
    "            one_news.extend(line)\n",
    "        if one_news != ['']:\n",
    "            All_Fake_Data.append(' '.join(one_news))\n",
    "    # get true data\n",
    "    for file in os.listdir(data_path_1_true):\n",
    "        file_path = data_path_1_true + file\n",
    "        data = open(file_path, encoding='utf-8', mode='r')\n",
    "        data = data.readlines()\n",
    "        one_news = []\n",
    "        for line in data:\n",
    "            line = clean_str(line.strip('\\n\\r')).split(' ')\n",
    "            one_news.extend(line)\n",
    "        if one_news != ['']:\n",
    "            All_True_Data.append(' '.join(one_news))\n",
    "print('all fake news length:', len(All_Fake_Data))\n",
    "print('all true news length:', len(All_True_Data))\n",
    "print('OK_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fake until 0k2: 699\n",
      "all true until 0k2: 701\n",
      "OK_2\n"
     ]
    }
   ],
   "source": [
    "# 2. get FakeNewsNet-master\n",
    "for data_set in ['BuzzFeed/', 'PolitiFact/']:\n",
    "    data_path_2 = data_file + 'FakeNewsNet-master/Data/' + data_set\n",
    "    data_path_2_fake = data_path_2 + 'FakeNewsContent/'\n",
    "    data_path_2_true = data_path_2 + 'RealNewsContent/'\n",
    "    for file in os.listdir(data_path_2_fake):\n",
    "        file_path = data_path_2_fake + file\n",
    "        data = open(file_path, encoding='utf-8', mode='r')\n",
    "        data = json.load(data)\n",
    "        news = data['text']\n",
    "        news = clean_str(news).split(' ')\n",
    "        if len(news) > 3:  # if the news is too short then we don't select it\n",
    "            All_Fake_Data.append(' '.join(news))\n",
    "\n",
    "    for file in os.listdir(data_path_2_true):\n",
    "        file_path = data_path_2_true + file\n",
    "        data = open(file_path, encoding='utf-8', mode='r')\n",
    "        data = json.load(data)\n",
    "        news = data['text']\n",
    "        news = clean_str(news).split(' ')\n",
    "        if len(news) > 3:\n",
    "            All_True_Data.append(' '.join(news))\n",
    "print('all fake until 0k2:', len(All_Fake_Data))\n",
    "print('all true until 0k2:', len(All_True_Data))\n",
    "print('OK_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linliu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fake until 0k3: 1565\n",
      "all true until 0k3: 2478\n",
      "OK_3\n"
     ]
    }
   ],
   "source": [
    "# 4. get PolitiFact\n",
    "politifact_path = data_file + '/politifact_data/full_politifact_data_1.csv'\n",
    "data = pd.read_csv(politifact_path, sep=None, error_bad_lines=False)\n",
    "\n",
    "claim = data[\"PolitiFact_explanations\"]\n",
    "body = data[\"Statement\"]\n",
    "claim_label = data[\"Truth-Rating\"]\n",
    "\n",
    "\n",
    "fake_index = [index for index, text in enumerate(claim_label) if int(text) == 5]\n",
    "non_fake_index = [index for index, text in enumerate(claim_label) if int(text) == 0]\n",
    "\n",
    "fake_claim = [text.lower() for index, text in enumerate(claim) if index in fake_index]\n",
    "non_claim = [text.lower() for index, text in enumerate(claim) if index in non_fake_index]\n",
    "\n",
    "fake_body = [text.lower() for index, text in enumerate(body) if index in fake_index]\n",
    "non_fake_body = [text.lower() for index, text in enumerate(body) if index in non_fake_index]\n",
    "\n",
    "for index, text in enumerate(fake_claim):\n",
    "    all_text = text + \" \" + fake_body[index]\n",
    "    text = clean_str(all_text)\n",
    "    All_Fake_Data.append(text)\n",
    "\n",
    "for index, text in enumerate(non_claim):\n",
    "    all_text = text + \" \" + non_fake_body[index]\n",
    "    text = clean_str(all_text)\n",
    "    All_True_Data.append(text)\n",
    "\n",
    "\n",
    "\n",
    "print('all fake until 0k3:', len(All_Fake_Data))\n",
    "print('all true until 0k3:', len(All_True_Data))\n",
    "print('OK_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fake until 0k4: 1584\n",
      "all true until 0k4: 2478\n",
      "OK_4\n"
     ]
    }
   ],
   "source": [
    "#### 5. get bs_detector\n",
    "bs_path = data_file + 'fake.csv'\n",
    "data = pd.read_csv(bs_path)\n",
    "claim = data[\"title\"]\n",
    "body = data[\"text\"]\n",
    "claim_label = data[\"type\"]\n",
    "\n",
    "fake_index = [index for index, text in enumerate(claim_label) if text.lower() == \"fake\"]\n",
    "fake_claim = [text.lower() for index, text in enumerate(claim) if index in fake_index]\n",
    "fake_body = [text.lower() for index, text in enumerate(body) if index in fake_index]\n",
    "\n",
    "for index, text in enumerate(fake_claim):\n",
    "    all_text = text + \" \" + fake_body[index]\n",
    "    text = clean_str(all_text)\n",
    "    All_Fake_Data.append(text)\n",
    "\n",
    "print('all fake until 0k4:', len(All_Fake_Data))\n",
    "print('all true until 0k4:', len(All_True_Data))\n",
    "print('OK_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fake until 0k5: 2135\n",
      "all true until 0k5: 3215\n",
      "OK_5\n"
     ]
    }
   ],
   "source": [
    "# 6. get emergent\n",
    "emergent_path = data_file + 'emergent.csv'\n",
    "\n",
    "data = pd.read_csv(emergent_path)\n",
    "claim = data[\"claim\"]\n",
    "body = data[\"body\"]\n",
    "claim_label = data[\"claim_label\"]\n",
    "\n",
    "fake_index = [index for index, text in enumerate(claim_label) if text.lower() == \"false\"]\n",
    "non_fake_index = [index for index, text in enumerate(claim_label) if text.lower() == \"true\"]\n",
    "\n",
    "fake_claim = [text.lower() for index, text in enumerate(claim) if index in fake_index]\n",
    "non_claim = [text.lower() for index, text in enumerate(claim) if index in non_fake_index]\n",
    "\n",
    "fake_body = [text.lower() for index, text in enumerate(body) if index in fake_index]\n",
    "non_fake_body = [text.lower() for index, text in enumerate(body) if index in non_fake_index]\n",
    "\n",
    "for index, text in enumerate(fake_claim):\n",
    "    all_text = text + \" \" + fake_body[index]\n",
    "    text = clean_str(all_text)\n",
    "    All_Fake_Data.append(text)\n",
    "\n",
    "for index, text in enumerate(non_claim):\n",
    "    all_text = text + \" \" + non_fake_body[index]\n",
    "    text = clean_str(all_text)\n",
    "    All_True_Data.append(text)\n",
    "\n",
    "print('all fake until 0k5:', len(All_Fake_Data))\n",
    "print('all true until 0k5:', len(All_True_Data))\n",
    "print('OK_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len All_Fake_Data: 2135\n",
      "len All_True_Data: 3215\n"
     ]
    }
   ],
   "source": [
    "print('len All_Fake_Data:', len(All_Fake_Data))\n",
    "print('len All_True_Data:', len(All_True_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fake_train, Fake_test = train_test_split(All_Fake_Data ,test_size=0.1, random_state=42)\n",
    "True_train, True_test = train_test_split(All_True_Data ,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/Fake_train','w')\n",
    "for i in Fake_train:\n",
    "    f.write(i+'\\n')\n",
    "f.close()\n",
    "f = open('./data/Fake_test','w')\n",
    "for i in Fake_test:\n",
    "    f.write(i+'\\n')\n",
    "f.close()\n",
    "f = open('./data/True_train','w')\n",
    "for i in True_train:\n",
    "    f.write(i+'\\n')\n",
    "f.close()\n",
    "f = open('./data/True_test','w')\n",
    "for i in True_test:\n",
    "    f.write(i+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
