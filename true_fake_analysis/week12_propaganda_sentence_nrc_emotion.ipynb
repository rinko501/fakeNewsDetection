{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linliu/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,f1_score,roc_auc_score,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get propaganda Data Begining ......\n",
      "7959 42297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "now... true_propaganda_words.csv\n",
      "['him', 'its', 'this', 'we', 'here', 'the', 'it', 'and', 'if', 'i', 'that', 'again', 'him', 'them', 'so', 'now', 'a', 'it', 'this', 'it', 'in', 'all', 'this', 'were', 'and', 'then', 'all', 'out', 'as', 'that', 'no', 'out', 'is', 'them', 'the', 'me', 'now', 'but', 'not', 'you', 'again', 'up', 'i', 'do', 'its', 'there', 'the', 'the', 'there', 'is', 'on', 'not', 'here', 'you', 'so', 'you', 'up', 'down', 'it', 'they', 'was', 'to', 'in', 'are', 'what', 'be', 'no', 'me', 'who', 'there', 'too', 'on', 'her', 'but', 'it', 'in', 'themselves', 'when', 'do', 'more', 'itself', 'and', 'or', 'in', 'did', 'are', 'down', 'i', 'he', 's', 'was', 'the', 'the', 'an', 'too', 'which', 'why', 'no', 'this', 'and', 'a', 'more', 'as', 'himself', 'for', 'before', 'her', 'it', 'this', 'that', 'which', 'himself', 'this', 'about', 'is', 'were', 'to', 'own', 'with', 'over', 'or', 'he', 'to', 'with', 'itself', 'for', 'so', 'not', 'but', 'be', 'themselves', 'that', 'our', 'other', 'before', 'my', 'of', 'all', 'it', 'because', 'for', 'i', 'the', 'in', 'we', 'did', 'off', 'him', 'they', 'a', 'off', 'about', 'them', 'will', 'were', 'does', 'other', 'on', 'not', 'm', 'how', 'd', 'will', 'here', 'same', 'why', 'them', 'as', 'above', 'what', 'was', 'further', 'it', 'have', 'were', 'you', 'a', 'these', 'have', 'own', 'am', 'at', 'below', 'just', 'has', 'is', 'doing', 'this', 'that', 'over', 'a', 'if', 'we', 'has', 'who', 'why', 'below', 'for', 'very', 'do', 'we', 'are', 'such', 'why', 'that', 'with', 'what', 'very', 'does', 'you', 'if', 'if', 'of', 'i', 'can', 'do', 'some', 'the', 'more', 'she', 'because', 'them', 'i', 'in', 'when', 'what', 'both', 'me', 'me', 'doing', 'from', 'this', 'him', 'that', 'to', 'most', 'here', 'him', 'who', 'i', 'i', 'were', 'then', 'but', 'out', 'it', 'will', 'that', 't', 'by', 'with', 'further', 'this', 'if', 'any', 'while', 'now', 'not', 'those', 'for', 'myself', 'after', 'from', 'now', 'them', 'it', 'there', 'are', 'you', 'to', 'by', 'he', 'we', 'i', 'as', 'it', 'a', 'how', 'when', 'to', 'over', 'now', 'an', 'such', 'being', 'that', 'it', 'i', 'while', 'can', 'he', 'some', 'and', 'his', 'after', 'been', 'once', 'them', 'it', 'what', 'herself', 'been', 'there', 'you', 'the', 'yourself', 'about', 'if', 'his', 'we', 'in', 'here', 'from', 'after', 'with', 'at', 'under', 'by', 'where', 'won', 'had', 'again', 'this', 'its', 'they', 'she', 'such', 'of', 'they', 'of', 'have', 'there', 'yourself', 'through', 'where', 'how', 'you', 'ourselves', 'all', 'be', 'at', 'above', 'being', 'me', 'it', 'has', 'these', 'their', 'while', 'he', 'your', 'herself', 'can', 'most', 'him', 'same', 'when', 'so', 'of', 'they', 'did', 'me', 'on', 'on', 'those', 'am', 'be', 'out', 'ourselves', 'against', 'him', 'only', 'to', 'a', 'of', 'to', 'no', 'these', 'there', 'there', 'below', 'here', 'now', 'up', 'at', 'up', 'had', 'do', 'in', 'on', 'only', 'out', 'here', 'is', 'through', 'in', 'from', 'as', 'the', 'had', 'only', 'up', 'all', 'up', 'again', 'all', 'no', 'myself', 'after', 'they', 'on', 'why', 'the', 'more', 'again', 'more', 'just', 'no', 'again', 'from', 'about', 'should', 'and', 'i', 'what', 'but', 'to', 'is', 'any', 'our', 'for', 'this', 'its', 'were', 'all', 'as', 'a', 'no', 'out', 'for', 'you', 'if', 'o', 'an', 'he', 'no', 'all', 'not', 'where', 'down', 'his', 'they', 'do', 'that', 'in', 'because', 'being', 'too', 'such', 'the', 'as', 'why', 'a', 'not', 'is', 'we', 'against', 'while', 'why', 'should', 'by', 'to', 'during', 'in', 'when', 'him', 'most', 'while', 'you', 'once', 'each', 'more', 'so', 'should', 'having', 'then', 'from', 'you', 'to', 'no', 'be', 'won', 'on', 'few', 'this', 'theirs', 'and', 'into', 'this', 'that', 'why', 'these', 'her', 'i', 'by', 'both', 'over', 'until', 'the', 'you', 'once', 'not', 'now', 'again', 'after', 'd', 'her', 'here', 'but', 'them', 'not', 'if', 'all', 'from', 'am', 'it', 'so', 'down', 'not', 'out', 'too', 'what', 'it', 'on', 'some', 'for', 'there', 'its', 'it', 'them', 'him', 'when', 'o', 'few', 'a', 'are', 'they', 'because', 'now', 'is', 'on', 'now', 'again', 'we', 'just', 'was', 'at', 'are', 'be', 'his', 'in', 'is', 'themselves', 'it', 'this', 'with', 'by', 'so', 'does', 'the', 'they', 'any', 'who', 't', 'it', 'were', 'about', 'no', 'out', 'here', 'its', 'yours', 'you', 'this', 'is', 'out', 'of', 'off', 'a', 'other', 'there', 'there', 'down', 'did', 'the', 'above', 'not', 'an', 'his', 'to', 'that', 'do', 'just', 'you', 'to', 'how', 'we', 'the', 'where', 'those']\n",
      "now... fake_propaganda_words.csv\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def clean_str(s):  # remove puctuation\n",
    "    s = re.sub('[^0-9\\w\\d\\s]+', '', s).replace('\\r',\" \").replace('\\n',\" \").replace('\\xa0',' ')\n",
    "    return s.strip().lower()\n",
    "\n",
    "def data_analysis(target_word_name=None):\n",
    "    # Data Analysis\n",
    "\n",
    "    word_list = {}\n",
    "\n",
    "    #  ************** Target Word List ******************\n",
    "    print(\"Get \"+target_word_name+\" Data Begining ......\")\n",
    "   \n",
    "    target_words_num = {}\n",
    "    target_words_list = []\n",
    "    target_words_weight = {}\n",
    "\n",
    "    if target_word_name == 'propaganda':\n",
    "\n",
    "        fake_pro_data = []\n",
    "        true_pro_data = []\n",
    "        \n",
    "        target_data_1 = open('../other_dict/task-1/task1.train.txt', encoding='utf-8', mode='r')\n",
    "        target_data_1 = target_data_1.readlines()\n",
    "        for line in target_data_1:\n",
    "            line = line.strip('\\n\\r').lower().split('\\t')\n",
    "            text = line[0]\n",
    "            label = line[2]\n",
    "            if label == 'propaganda':\n",
    "                true_pro_data.append(text)\n",
    "            else:\n",
    "                fake_pro_data.append(text)\n",
    "                \n",
    "        tesk_2_3_path = '../other_dict/tasks-2-3/train/'\n",
    "        target_data_2_dirs = os.listdir(tesk_2_3_path)\n",
    "        article_index_set = set()\n",
    "        for file_name in target_data_2_dirs:\n",
    "            an_article_index = file_name.split(\".\")[0] \n",
    "            article_index_set.add(an_article_index)\n",
    "        #print(\"article_index_set:\",article_index_set)\n",
    "\n",
    "        for index in article_index_set:\n",
    "            text_name = tesk_2_3_path + index + '.txt'\n",
    "            try:\n",
    "                text = open(text_name, encoding='utf-8', mode='r')\n",
    "            except:\n",
    "                print('No such file or directory: ', text_name)\n",
    "                continue\n",
    "            try:\n",
    "                label_name = tesk_2_3_path + index + '.task2.labels'\n",
    "                label = open(label_name, encoding='utf-8', mode='r')\n",
    "            except:\n",
    "                print('No such file or directory: ', label_name)\n",
    "                continue\n",
    "            text_list = []\n",
    "            for line in text.readlines():\n",
    "                text_list.append(line)\n",
    "            label = label.readlines()\n",
    "            label_list = []\n",
    "            for line in label:\n",
    "                label_list.append(line)\n",
    "            for i in range(len(text_list)):\n",
    "                #print(label_list[i])\n",
    "                if i < len(label_list):\n",
    "                    if text_list[i].strip() != '':\n",
    "                        if \"\\tpropaganda\" in label_list[i]:\n",
    "                            true_pro_data.append(text_list[i])\n",
    "                        elif \"\\tnon-propaganda\" in label_list[i]:\n",
    "                            fake_pro_data.append(text_list[i])\n",
    "                        # else:\n",
    "                            # print(label_list[i])\n",
    "                            # print(\"No Match !!!\", index, i)\n",
    "        print(len(true_pro_data),len(fake_pro_data))\n",
    "\n",
    "       \n",
    "        fake_pro_num = 0 \n",
    "        true_pro_num = 0\n",
    "        word_list = {}\n",
    "        for line in fake_pro_data:\n",
    "            line_word_list = []\n",
    "            fake_pro_num += 1\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            for word in line:\n",
    "                word=clean_str(word)\n",
    "                if word in line_word_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    line_word_list.append(word)\n",
    "                    if word in word_list.keys():\n",
    "                        word_list[word][0] += 1\n",
    "                    else:\n",
    "                        word_list[word] = [1, 0]\n",
    "\n",
    "        for line in true_pro_data:\n",
    "            line_word_list = []\n",
    "            true_pro_num += 1\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            for word in line:\n",
    "                if word in line_word_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    line_word_list.append(word)\n",
    "                    if word in word_list.keys():\n",
    "                        word_list[word][1] += 1\n",
    "                    else:\n",
    "                        word_list[word] = [0, 1]\n",
    "        \n",
    "        import nltk\n",
    "        nltk.download('stopwords')\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = stopwords.words('english')\n",
    "        fake_word_dict={}\n",
    "        true_word_dict={}\n",
    "        for word, v in word_list.items():\n",
    "            word_in_fake = v[0]\n",
    "            word_in_true = v[1]\n",
    "            if word_in_fake + word_in_true > 10 and word not in stop_words:\n",
    "                # print(word_in_fake, word_in_true)\n",
    "                word_not_in_fake = fake_pro_num - word_in_fake\n",
    "                word_not_in_true = true_pro_num - word_in_true\n",
    "                obs = np.array([[word_in_fake, word_not_in_fake], [word_in_true, word_not_in_true]])\n",
    "                g, p, dof, expctd = chi2_contingency(obs)\n",
    "                if(p < 0.05): \n",
    "                    if word_in_fake > word_in_true: \n",
    "                        fake_word_dict[word] = p\n",
    "                    else:\n",
    "                        true_word_dict[word] = p\n",
    "\n",
    "    \n",
    "    #  **************************************************\n",
    "\n",
    "   \n",
    "   \n",
    "        \n",
    "\n",
    "    fake_word_dict = sorted(fake_word_dict.items(), key=lambda d: d[1], reverse=False)  \n",
    "    true_word_dict = sorted(true_word_dict.items(), key=lambda d: d[1], reverse=False)\n",
    "\n",
    "\n",
    "   \n",
    "    fake_target_words_path = './result_fake_true_week5/fake_' + target_word_name + '_words.csv'\n",
    "    with open(fake_target_words_path, \"w\") as fake_key_words:\n",
    "        writer = csv.writer(fake_key_words)\n",
    "        for i, item in enumerate(fake_word_dict):\n",
    "            clean_word = clean_str(item[0])\n",
    "            try:\n",
    "                word_weight = target_words_weight[clean_word]\n",
    "            except:\n",
    "                word_weight = 'None'\n",
    "            writer.writerow([clean_word, item[1], word_weight])\n",
    "\n",
    "    true_target_words_path = './result_fake_true_week5/true_' + target_word_name + '_words.csv'\n",
    "    with open(true_target_words_path, \"w\") as true_key_words:\n",
    "        writer = csv.writer(true_key_words)\n",
    "        for i, item in enumerate(true_word_dict):\n",
    "            clean_word = clean_str(item[0])\n",
    "            try:\n",
    "                word_weight = target_words_weight[clean_word]\n",
    "            except:\n",
    "                word_weight = 'None'\n",
    "            writer.writerow([clean_word, item[1], word_weight])\n",
    "\n",
    "\n",
    "\n",
    "data_analysis(\"propaganda\")\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "words = stopwords.words('english')\n",
    "print(words)\n",
    "import os\n",
    "path = \"./result_fake_true_week5/\"\n",
    "files= os.listdir(path) \n",
    "s = []\n",
    "for file in files: \n",
    "     if not os.path.isdir(file): \n",
    "        print(\"now...\",file)\n",
    "        if(file == '.DS_Store'):\n",
    "            continue\n",
    "        f = open(path+\"/\"+file); \n",
    "        iter_f = iter(f); \n",
    "        fw = open(\"./result_fake_true_reduce_stopwords_week5/\"+file,'w+')\n",
    "        reduce_list = []\n",
    "        for line in iter_f: \n",
    "            word = line.split(',')[0]\n",
    "            if word not in words:\n",
    "                fw.write(line)\n",
    "            else:\n",
    "                reduce_list.append(word)\n",
    "        print(reduce_list)\n",
    "        f.close()\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_propaganda_task2():\n",
    "    '''\n",
    "    output:[[sentence, label]]\n",
    "    '''\n",
    "    global_sentence_label = list()\n",
    "    \n",
    "    tesk_2_3_path = '../other_dict/tasks-2-3/train/'\n",
    "    target_data_2_dirs = os.listdir(tesk_2_3_path)\n",
    "    article_name_set = set()\n",
    "    for file_name in target_data_2_dirs:\n",
    "        article_name_set.add(file_name.split(\".\")[0])\n",
    "    for index in list(article_name_set):\n",
    "        text_name = tesk_2_3_path + index + '.txt'\n",
    "        try:\n",
    "            text = open(text_name, encoding='utf-8', mode='r')\n",
    "        except:\n",
    "            print('No such file or directory: ', text_name)\n",
    "            continue\n",
    "        label_name = tesk_2_3_path + index + '.task2.labels'\n",
    "        try:\n",
    "            label = open(label_name, encoding='utf-8', mode='r')\n",
    "        except:\n",
    "            print('No such file or directory: ', label_name)\n",
    "            continue\n",
    "        text = text.readlines()\n",
    "        sentence_list = []\n",
    "        for sentence in text:\n",
    "            sentence_list.append(sentence)\n",
    "        label = label.readlines()\n",
    "        label_list = []\n",
    "        for line in label:\n",
    "            label_list.append(line)\n",
    "        assert len(label_list)==len(sentence_list),\"text name %s, len of text %d, \\\n",
    "                len of label %d\"%(text_name, len(sentence_list), len(label_list))\n",
    "        for index,sentence in enumerate(sentence_list):\n",
    "            if \"\\tnon-propaganda\" in label_list[index] and sentence.strip()!=\"\":\n",
    "                global_sentence_label.append([sentence.strip(),0])\n",
    "            elif \"\\tpropaganda\" in label_list[index] and sentence.strip()!=\"\":\n",
    "                global_sentence_label.append([sentence.strip(),1])\n",
    "            else:\n",
    "                continue\n",
    "    return global_sentence_label\n",
    "# load data\n",
    "global_sentence_label = load_propaganda_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_dict = dict()\n",
    "neg_word_dict = dict()\n",
    "pos_num = 0\n",
    "neg_num = 0\n",
    "for ele in global_sentence_label:\n",
    "    if ele[1]==1:\n",
    "        pos_num += 1\n",
    "        word_list = set(ele[0].lower().split(\" \"))\n",
    "        for word in word_list:\n",
    "            if word in pos_word_dict:\n",
    "                pos_word_dict[word] += 1\n",
    "            else:\n",
    "                pos_word_dict[word] = 1\n",
    "    else:\n",
    "        neg_num += 1\n",
    "        word_list = set(ele[0].split(\" \"))\n",
    "        for word in word_list:\n",
    "            if word in neg_word_dict:\n",
    "                neg_word_dict[word] += 1\n",
    "            else:\n",
    "                neg_word_dict[word] = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_set = set(pos_word_dict.keys())\n",
    "neg_word_set = set(neg_word_dict.keys())\n",
    "all_word_set = pos_word_set.union(neg_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_significant_word_set = set()\n",
    "neg_significant_word_set = set()\n",
    "for word in all_word_set:\n",
    "    if word in pos_word_dict.keys():\n",
    "        word_in_pos = pos_word_dict[word]\n",
    "    else:\n",
    "        word_in_pos = 0\n",
    "    if word in neg_word_dict.keys():\n",
    "        word_in_neg = neg_word_dict[word]\n",
    "    else:\n",
    "        word_in_neg = 0\n",
    "        \n",
    "    if word_in_pos + word_in_neg > 10:\n",
    "        word_not_in_pos = pos_num - word_in_pos\n",
    "        word_not_in_neg = neg_num - word_in_neg\n",
    "        g, p, dof, expctd = chi2_contingency(np.array([[word_in_pos, word_not_in_pos],[word_in_neg, word_not_in_neg]]))\n",
    "        if p < 0.05:\n",
    "            if word_in_pos > word_in_neg:\n",
    "                pos_significant_word_set.add(word)\n",
    "            else:\n",
    "                neg_significant_word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of pos significant words 491, num of neg significant word 792\n"
     ]
    }
   ],
   "source": [
    "print(\"num of pos significant words %d, num of neg significant word %d\"%(len(pos_significant_word_set), len(neg_significant_word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get pos_nrc_emotion Data Begining ......\n",
      "len of pos nrm word dict is  113\n",
      "Get neg_nrc_emotion Data Begining ......\n",
      "len of neg nrm word dict is  62\n"
     ]
    }
   ],
   "source": [
    "target_word_name='nrc_emotion'\n",
    "print(\"Get pos_\"+target_word_name+\" Data Begining ......\")\n",
    "nrc_word_dict = dict()\n",
    "nrc_word_set=set()\n",
    "if target_word_name == 'nrc_emotion':\n",
    "        target_data = open('../other_dict/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', encoding='utf-8', mode='r')\n",
    "        target_data = target_data.readlines()\n",
    "        for line in target_data:\n",
    "            line = line.strip('\\n\\r').lower().split('\\t')\n",
    "            if len(line)==3 and line[2]== \"1\" and line[0] in pos_significant_word_set:\n",
    "              \n",
    "                nrc_word_set.add(line[0])\n",
    "        for word in nrc_word_set:\n",
    "            nrc_word_dict[word] = 1            \n",
    "print(\"len of pos nrm word dict is \", len(nrc_word_dict))\n",
    "\n",
    "new_target_words_path = './result_fake_true_after_reduce_stopwords/true_' + target_word_name + '_words.csv'\n",
    "with open(new_target_words_path, \"w\") as new_key_words:\n",
    "    writer = csv.writer(new_key_words)\n",
    "    for key in nrc_word_dict.keys():\n",
    "        word = key\n",
    "        try:\n",
    "            word_weight = nrc_word_dict[word]\n",
    "        except:\n",
    "            word_weight = 'None'\n",
    "        writer.writerow([word, word_weight])\n",
    "\n",
    "target_word_name='nrc_emotion'\n",
    "print(\"Get neg_\"+target_word_name+\" Data Begining ......\")\n",
    "nrc_word_dict = dict()\n",
    "nrc_word_set=set()\n",
    "if target_word_name == 'nrc_emotion':\n",
    "        target_data = open('../other_dict/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', encoding='utf-8', mode='r')\n",
    "        target_data = target_data.readlines()\n",
    "        for line in target_data:\n",
    "            line = line.strip('\\n\\r').lower().split('\\t')\n",
    "            if len(line)==3 and line[2]== \"1\" and line[0] in neg_significant_word_set:\n",
    "                nrc_word_set.add(line[0])\n",
    "        for word in nrc_word_set:\n",
    "            nrc_word_dict[word] = 1            \n",
    "print(\"len of neg nrm word dict is \", len(nrc_word_dict))\n",
    "\n",
    "new_target_words_path = './result_fake_true_after_reduce_stopwords/fake_' + target_word_name + '_words.csv'\n",
    "with open(new_target_words_path, \"w\") as new_key_words:\n",
    "    writer = csv.writer(new_key_words)\n",
    "    for key in nrc_word_dict.keys():\n",
    "        word = key\n",
    "        try:\n",
    "            word_weight = nrc_word_dict[word]\n",
    "        except:\n",
    "            word_weight = 'None'\n",
    "        writer.writerow([word, word_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of sentence is 4032\n",
      "num of pos sentence is 1239\n",
      "num of neg sentence is 2793\n"
     ]
    }
   ],
   "source": [
    "global_test_sentence_label = list()\n",
    "sentence_test_list = []\n",
    "label_test_list = []\n",
    "with open('../other_dict/new_propaganda_test/task2test_rs.csv','r') as csvfile_test_test:\n",
    "    reader_test = csv.reader(csvfile_test_test)\n",
    "    for i,rows in enumerate(reader_test):\n",
    "        row_test = rows\n",
    "        text_test = row_test[3]\n",
    "        label =row_test[2]\n",
    "        if label==\"non-propaganda\":\n",
    "            global_test_sentence_label.append([text_test,0])\n",
    "        elif label==\"propaganda\":\n",
    "            global_test_sentence_label.append([text_test,1])\n",
    "        else:\n",
    "            continue\n",
    "# partition to positive/negative\n",
    "global_test_sentence_label_positive = [pair for pair in global_test_sentence_label if pair[1]==1]\n",
    "global_test_sentence_label_negative = [pair for pair in global_test_sentence_label if pair[1]==0]\n",
    "print(\"num of sentence is %d\"%(len(global_test_sentence_label)))\n",
    "print(\"num of pos sentence is %d\"%(len(global_test_sentence_label_positive)))\n",
    "print(\"num of neg sentence is %d\"%(len(global_test_sentence_label_negative)))\n",
    "test_dataset = global_test_sentence_label_negative[:]\n",
    "test_dataset.extend(global_test_sentence_label_positive[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of sentence is 14263\n",
      "num of pos sentence is 3938\n",
      "num of neg sentence is 3938\n"
     ]
    }
   ],
   "source": [
    "def load_propaganda_task2():\n",
    "    '''\n",
    "    output:[[sentence, label]]\n",
    "    '''\n",
    "    global_sentence_label = list()\n",
    "    \n",
    "    tesk_2_3_path = '../other_dict/tasks-2-3/train/'\n",
    "    target_data_2_dirs = os.listdir(tesk_2_3_path)\n",
    "    article_name_set = set()\n",
    "    for file_name in target_data_2_dirs:\n",
    "        article_name_set.add(file_name.split(\".\")[0])\n",
    "    for index in list(article_name_set):\n",
    "        text_name = tesk_2_3_path + index + '.txt'\n",
    "        try:\n",
    "            text = open(text_name, encoding='utf-8', mode='r')\n",
    "        except:\n",
    "            print('No such file or directory: ', text_name)\n",
    "            continue\n",
    "        label_name = tesk_2_3_path + index + '.task2.labels'\n",
    "        try:\n",
    "            label = open(label_name, encoding='utf-8', mode='r')\n",
    "        except:\n",
    "            print('No such file or directory: ', label_name)\n",
    "            continue\n",
    "        text = text.readlines()\n",
    "        sentence_list = []\n",
    "        for sentence in text:\n",
    "            sentence_list.append(sentence)\n",
    "        label = label.readlines()\n",
    "        label_list = []\n",
    "        for line in label:\n",
    "            label_list.append(line)\n",
    "        assert len(label_list)==len(sentence_list),\"text name %s, len of text %d, \\\n",
    "                len of label %d\"%(text_name, len(sentence_list), len(label_list))\n",
    "        for index,sentence in enumerate(sentence_list):\n",
    "            if \"\\tnon-propaganda\" in label_list[index] and sentence.strip()!=\"\":\n",
    "                global_sentence_label.append([sentence,0])\n",
    "            elif \"\\tpropaganda\" in label_list[index] and sentence.strip()!=\"\":\n",
    "                global_sentence_label.append([sentence,1])\n",
    "            else:\n",
    "                continue\n",
    "    return global_sentence_label\n",
    "# load data\n",
    "global_sentence_label = load_propaganda_task2()\n",
    "np.random.shuffle(global_sentence_label)\n",
    "# partition to positive/negative\n",
    "global_sentence_label_positive = [pair for pair in global_sentence_label if pair[1]==1]\n",
    "global_sentence_label_negative = [pair for pair in global_sentence_label if pair[1]==0][:3938]\n",
    "print(\"num of sentence is %d\"%(len(global_sentence_label)))\n",
    "print(\"num of pos sentence is %d\"%(len(global_sentence_label_positive)))\n",
    "print(\"num of neg sentence is %d\"%(len(global_sentence_label_negative)))\n",
    "train_dataset = global_sentence_label_negative[:]\n",
    "train_dataset.extend(global_sentence_label_positive[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    word_set = set()\n",
    "    dict_text = open(dict_path, encoding='utf-8', mode='r')\n",
    "    for line in dict_text:\n",
    "        if line.strip()!=\"\":\n",
    "            word =line.split(\",\")[0]\n",
    "            word_set.add(word)\n",
    "    return word_set\n",
    "def word_num_in_dict(sentence, word_set):\n",
    "    try:\n",
    "        word_list = sentence.strip().split(\" \")\n",
    "    except:\n",
    "        print(sentence)\n",
    "    word_num = 0\n",
    "    for word in word_list:\n",
    "        if word in word_set:\n",
    "            word_num += 1\n",
    "    return word_num\n",
    "def build_feature_matrix_and_label(dataset,dict_names,dict_path):\n",
    "    num_sample = len(dataset)\n",
    "    num_feature = len(dict_names)\n",
    "    feature_matrix = np.zeros((num_sample, num_feature))\n",
    "    label = np.zeros((num_sample,))\n",
    "    for i in range(num_sample):\n",
    "        for j in range(num_feature):\n",
    "            if 'propaganda' in dict_names[j]:\n",
    "                dict_path='./result_fake_true_week5/'\n",
    "            else:\n",
    "                dict_path=\"./result_fake_true_after_reduce_stopwords/\"\n",
    "            feature_matrix[i,j] = word_num_in_dict(dataset[i][0], load_dict(dict_path + dict_names[j] + \"_words.csv\"))\n",
    "        label[i] = dataset[i][1]\n",
    "    return feature_matrix,label\n",
    "def build_feature_matrix_new_one_dict(dataset,dict_name,dict_path):\n",
    "    if 'propaganda' in dict_name:\n",
    "        dict_path='./result_fake_true_week5/'\n",
    "    else:\n",
    "        dict_path=\"./result_fake_true_after_reduce_stopwords/\"\n",
    "    word_list = list(load_dict(dict_path+dict_name+ \"_words.csv\"))\n",
    "    num_sample = len(dataset)\n",
    "    num_feature = len(word_list)\n",
    "    feature_matrix = np.zeros((num_sample, num_feature))\n",
    "    for i in range(len(dataset)):\n",
    "        for j,word in enumerate(word_list):\n",
    "            if word in dataset[i][0]:\n",
    "                feature_matrix[i][j]=1\n",
    "    return(feature_matrix)\n",
    "\n",
    "dict_names = ['fake_persuasive','fake_sentiment','fake_nrc_emotion','fake_subjectivity','fake_technical','fake_all',\n",
    "              'true_persuasive','true_sentiment','true_nrc_emotion','true_subjectivity','true_technical','true_all']\n",
    "dict_path = \"./result_fake_true_after_reduce_stopwords/\"\n",
    "train_feature_matrix, train_label = build_feature_matrix_and_label(train_dataset, dict_names, dict_path)\n",
    "test_feature_matrix, test_label = build_feature_matrix_and_label(test_dataset, dict_names, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot \n",
    "for dict_name in dict_names:\n",
    "    train_feature_matrix=np.hstack((build_feature_matrix_new_one_dict(train_dataset,dict_name,dict_path),train_feature_matrix))\n",
    "    test_feature_matrix=np.hstack((build_feature_matrix_new_one_dict(test_dataset,dict_name,dict_path),test_feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (7876, 1437) (7876,) (4032, 1437) (4032,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape\",train_feature_matrix.shape, train_label.shape, test_feature_matrix.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train================================\n",
      "The train accuracy score of rf is : 0.620620\n",
      "confusion matrix is  [[3102 2152]\n",
      " [ 836 1786]]\n",
      " positive f1: 0.5445121951219513\n",
      " negative f1: 0.674934725848564\n",
      "The AUC of GBDT: 0.67317\n",
      "test================================\n",
      "The test accuracy score of rf is : 0.671379\n",
      "confusion matrix is  [[2239  771]\n",
      " [ 554  468]]\n",
      " positive f1: 0.41397611676249446\n",
      " negative f1: 0.7716698259520938\n",
      "f1: 0.41397611676249446\n",
      "precision: 0.45792563600782776\n",
      "recall: 0.37772397094430993\n",
      "The AUC of GBDT: 0.62666\n"
     ]
    }
   ],
   "source": [
    "#gbdt, train\n",
    "gbdt = GradientBoostingClassifier(max_depth=4,\n",
    "                                  random_state=0,\n",
    "                                  min_samples_split=5,\n",
    "                                  learning_rate=0.01,\n",
    "                                  n_estimators=30,\n",
    "                                  subsample=0.8)\n",
    "\n",
    "sample_weight = np.zeros(train_label.shape,np.float)\n",
    "sample_weight[train_label==1] = 1.0 / np.sum(train_label==1)\n",
    "sample_weight[train_label==0] = 1.0 / np.sum(train_label==0)\n",
    "rf = gbdt.fit(train_feature_matrix, train_label, sample_weight)\n",
    "\n",
    "print(\"train================================\")\n",
    "\n",
    "val_score_rbf = gbdt.score(train_feature_matrix, train_label)#val_score_rbf = gbdt.score(x, y)\n",
    "print(\"The train accuracy score of rf is : %f\" % val_score_rbf)\n",
    "predict_label = gbdt.predict(train_feature_matrix)\n",
    "prob_y = gbdt.predict_proba(train_feature_matrix)[:,1]\n",
    "cm = confusion_matrix(predict_label,train_label)\n",
    "print(\"confusion matrix is \", cm)\n",
    "#plot_confusion_matrix(cm, \"GBDT Confusion Matrix\")\n",
    "print(\" positive f1:\",f1_score(train_label,predict_label,pos_label=True))\n",
    "print(\" negative f1:\",f1_score(train_label,predict_label,pos_label=False))\n",
    "#print('test_feature_matrix:',test_feature_matrix)\n",
    "gbdt_auc = roc_auc_score(train_label, prob_y)\n",
    "print('The AUC of GBDT: %.5f' % gbdt_auc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"test================================\")\n",
    "test_score_rbf = gbdt.score(test_feature_matrix, test_label)#test_score_rbf = gbdt.score(test_x, test_y)\n",
    "print(\"The test accuracy score of rf is : %f\" % test_score_rbf)\n",
    "predict_label = gbdt.predict(test_feature_matrix)\n",
    "prob_y = gbdt.predict_proba(test_feature_matrix)[:,1]\n",
    "cm = confusion_matrix(predict_label,test_label)\n",
    "print(\"confusion matrix is \", cm)\n",
    "#plot_confusion_matrix(cm, \"GBDT Confusion Matrix\")\n",
    "\n",
    "\n",
    "print(\" positive f1:\",f1_score(test_label,predict_label,pos_label=True))\n",
    "print(\" negative f1:\",f1_score(test_label,predict_label,pos_label=False))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "f1 = f1_score(test_label, predict_label, average='binary')\n",
    "print(\"f1:\", f1 )\n",
    "p = precision_score(test_label, predict_label, average='binary')\n",
    "print(\"precision:\", p)\n",
    "r = recall_score(test_label, predict_label,average='binary')\n",
    "print(\"recall:\", r)\n",
    "\n",
    "\n",
    "#print('test_feature_matrix:',test_feature_matrix)\n",
    "gbdt_auc = roc_auc_score(test_label, prob_y)\n",
    "print('The AUC of GBDT: %.5f' % gbdt_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
